{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Packages"
      ],
      "metadata": {
        "id": "PDVTO9XBaEhM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ5fzGo2mWNj",
        "outputId": "24eb4c6f-2ca9-439e-8b3c-ca0939f68000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning-utilities\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities) (4.15.0)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities\n",
            "Successfully installed lightning-utilities-0.15.2\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-1.8.2\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.0.6)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# Install torch metrics package for LOSS, EVALUATION METRICS\n",
        "!pip install lightning-utilities\n",
        "!pip install torchmetrics --no-deps\n",
        "!pip install torchinfo\n",
        "!pip install -U albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0sgYUzXmGkC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "from torchmetrics import MeanMetric, Accuracy\n",
        "from torchmetrics import ConfusionMatrix, Precision, Recall, F1Score\n",
        "from torchinfo import summary\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import albumentations as alb\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import gc\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GPxBoQXjChW",
        "outputId": "565afa11-52fb-442b-8d66-3bc9e2aa06a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.get_device_name() if torch.cuda.is_available() else \"No GPU detected\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7ZAsdhZTaaB",
        "outputId": "a0f9e237-ec02-432c-c7f5-4153dbc0e50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4uBrs_dPHGH"
      },
      "source": [
        "#Albumentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcLulBTZ8uiV"
      },
      "source": [
        "resizing to 384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrDcflAovu9A",
        "outputId": "7073a73b-7f94-430d-b710-7f36648bfc3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-673664449.py:9: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
            "  alb.CoarseDropout(max_holes=2, max_height=18, max_width=18, p=0.3),\n",
            "/tmp/ipython-input-673664449.py:20: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
            "  alb.CoarseDropout(max_holes=2, max_height=18, max_width=18, p=0.3),\n",
            "/tmp/ipython-input-673664449.py:32: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
            "  alb.CoarseDropout(max_holes=2, max_height=18, max_width=18, p=0.3),\n",
            "/tmp/ipython-input-673664449.py:42: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
            "  alb.CoarseDropout(max_holes=2, max_height=18, max_width=18, p=0.3),\n"
          ]
        }
      ],
      "source": [
        "import albumentations as alb\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "train_augmentations = {\n",
        "    \"DF\": alb.Compose([\n",
        "        alb.HorizontalFlip(p=0.7),\n",
        "        alb.Rotate(limit=270, p=0.7),\n",
        "        alb.RandomBrightnessContrast(p=0.7),\n",
        "        alb.CoarseDropout(max_holes=2, max_height=18, max_width=18, p=0.3),\n",
        "        alb.GaussianBlur(p=0.2),\n",
        "        alb.Resize(384,384),\n",
        "        alb.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]),\n",
        "    \"VASC\": alb.Compose([\n",
        "        alb.HorizontalFlip(p=0.7),\n",
        "        alb.Rotate(limit=270, p=0.7),\n",
        "        alb.RandomBrightnessContrast(p=0.7),\n",
        "        alb.CoarseDropout(max_holes=2, max_height=18, max_width=18, p=0.3),\n",
        "        alb.GaussianBlur(p=0.2),\n",
        "        alb.Resize(384,384),\n",
        "        alb.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "\n",
        "    ]),\n",
        "    \"AK\": alb.Compose([\n",
        "        alb.HorizontalFlip(p=0.7),\n",
        "        alb.Rotate(limit=270, p=0.7),\n",
        "        alb.RandomBrightnessContrast(p=0.7),\n",
        "        alb.CoarseDropout(max_holes=2, max_height=18, max_width=18, p=0.3),\n",
        "        alb.Resize(384,384),\n",
        "        alb.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]),\n",
        "    \"SCC\": alb.Compose([\n",
        "        alb.HorizontalFlip(p=0.7),\n",
        "        alb.Rotate(limit=270, p=0.7),\n",
        "        alb.RandomBrightnessContrast(p=0.3),\n",
        "        alb.CoarseDropout(max_holes=2, max_height=18, max_width=18, p=0.3),\n",
        "        alb.GaussianBlur(p=0.2),\n",
        "        alb.Resize(384,384),\n",
        "        alb.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]),\n",
        "    \"BKL\":alb.Compose([\n",
        "        alb.HorizontalFlip(p=0.5),\n",
        "        alb.Rotate(limit=270, p=0.5),\n",
        "        alb.RandomBrightnessContrast(p=0.5),\n",
        "        alb.Resize(384,384),\n",
        "        alb.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]),\n",
        "    \"BCC\":alb.Compose([\n",
        "        alb.HorizontalFlip(p=0.3),\n",
        "        alb.Rotate(limit=270, p=0.5),\n",
        "        alb.RandomBrightnessContrast(p=0.5),\n",
        "        alb.Resize(384,384),\n",
        "        alb.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]),\n",
        "    \"MEL\":alb.Compose([\n",
        "        alb.HorizontalFlip(p=0.3),\n",
        "        alb.RandomBrightnessContrast(p=0.4),\n",
        "        alb.Resize(384,384),\n",
        "        alb.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]),\n",
        "    \"NV\":alb.Compose([\n",
        "        alb.Resize(384,384),\n",
        "        alb.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxl1k1mdDnF1"
      },
      "outputs": [],
      "source": [
        "class AlbTransform:\n",
        "    def __init__(self, augmentation_dict):\n",
        "        self.augmentation_dict = augmentation_dict\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = np.array(img)  # Convert PIL Image to NumPy array\n",
        "        label = img.filename.split(\"/\")[-2]  # Extract label from folder structure\n",
        "\n",
        "        if label in self.augmentation_dict:\n",
        "            augmented = self.augmentation_dict[label](image=img)[\"image\"]\n",
        "        else:\n",
        "            augmented = img  # No augmentation applied\n",
        "\n",
        "        tensor_img = ToTensorV2()(image=augmented)[\"image\"]\n",
        "        tensor_img = tensor_img.permute(2, 0, 1).contiguous()  # Fix channel order\n",
        "\n",
        "        return tensor_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlhzzakPPWtf"
      },
      "source": [
        "# Train/Val Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xblY4ej-SjAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb64bc84-f589-4920-c23d-914a66acb62c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 1 =====\n",
            "\n",
            "===== Fold 2 =====\n",
            "\n",
            "===== Fold 3 =====\n",
            "\n",
            "===== Fold 4 =====\n",
            "\n",
            "===== Fold 5 =====\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Subset\n",
        "from collections import Counter\n",
        "import torch\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Load entire dataset once\n",
        "dataset = ImageFolder(root=\"/content/drive/MyDrive/ISIC_2019_raw\", transform=None)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=27)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.arange(len(dataset)), dataset.targets)):\n",
        "    print(f\"\\n===== Fold {fold+1} =====\")\n",
        "    train_set = Subset(dataset, train_idx)\n",
        "    val_set   = Subset(dataset, val_idx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LomxokYJsl4",
        "outputId": "8436ced9-d589-4bd5-8d15-9ebfd5001e6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Classes: 8\n"
          ]
        }
      ],
      "source": [
        " # Define classes\n",
        "classes = [\"AK\", \"BCC\", \"BKL\",\"DF\",\"MEL\",\"NV\",\"SCC\",\"VASC\"]\n",
        "num_class=len(classes)\n",
        "print(\"Number of Classes:\", num_class)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = Counter([dataset.targets[i] for i in train_idx])\n",
        "weights = np.array([1.0 / class_counts[c] for c in range(len(classes))], dtype=np.float32)\n",
        "class_weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "QcPZdNtrYZSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe50VEv5HU1g"
      },
      "outputs": [],
      "source": [
        "# Sampler\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "sample_weights = torch.tensor([weights[dataset.targets[i]] for i in train_idx],dtype=torch.float)\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez2ZZLeHVRcj"
      },
      "outputs": [],
      "source": [
        "train_set.dataset.transform = AlbTransform(train_augmentations)  # Augmentations only for training\n",
        "val_set.dataset.transform = transforms.Compose([\n",
        "    transforms.Resize((384,384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])  #  No augmentations for validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN60uGhLEFeG"
      },
      "outputs": [],
      "source": [
        "test_set=ImageFolder(root=\"/content/drive/MyDrive/ISIC_2019_test_raw\", transform=transforms.Compose([\n",
        "    transforms.Resize((384,384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reduced from 64 as increased computational power for resizing to 384x384"
      ],
      "metadata": {
        "id": "VlrpipYhbhSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qau9X1FjVdrW"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=8, sampler=sampler)\n",
        "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)\n",
        "test_loader=DataLoader(test_set, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUIXSiKaeZA_",
        "outputId": "c5e4859d-fa46-4ef1-8493-df749a8cee29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train Samples: 20265\n",
            "Number of Validation Samples: 5066\n",
            "Number of Test Samples: 8238\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of Train Samples:\", len(train_set))\n",
        "print(\"Number of Validation Samples:\", len(val_set))\n",
        "print(\"Number of Test Samples:\", len(test_set))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDaXLwetelqn",
        "outputId": "5d9be63e-7a23-4111-f9f1-2140afc5eab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Classes: 8\n"
          ]
        }
      ],
      "source": [
        " # Define classes\n",
        "classes = [\"AK\", \"BCC\", \"BKL\",\"DF\",\"MEL\",\"NV\",\"SCC\",\"VASC\"]\n",
        "num_class=len(classes)\n",
        "print(\"Number of Classes:\", num_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp_B9tP6gvxP"
      },
      "source": [
        "#Pretrained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf5n8_uJnuTM"
      },
      "source": [
        "Next steps\n",
        "1. Load Pretrained Models\n",
        "2. Maybe make my own?/Contrastive Learning Model and compare\n",
        "    - MoCo (classifier fine tune)\n",
        "    - would need contrastive learning loss function for pretraining\n",
        "    - Compare contrastive embeddings vs CNN\n",
        "\n",
        "3. Evaluate Metrics F1, acc, prec, auc\n",
        "4. Grad-CAM/SHAP interpretability inspected\n",
        "\n",
        "5. Fusion Model using patient metadata\n",
        "6. Evaluation metrics F1, acc,prec, auc\n",
        "7. Grad-CAM/SHAP intrepretability inspected\n",
        "\n",
        "8. Compare Overall results\n",
        "  - Img ONLY vs Img + Metadata:\n",
        "    - F1 scores, reliability (precision)\n",
        "    - Interpretability (heatmap)\n",
        "    - Interpretability (SHAP)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYUEF6sRhVGm"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kjyYp-mSmpe"
      },
      "outputs": [],
      "source": [
        "project_root = \"/content/drive/MyDrive/Thesis/Updated MODELS\"\n",
        "os.makedirs(project_root, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score, MulticlassAUROC"
      ],
      "metadata": {
        "id": "HcUwB7keLVKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcX4WIIpgzEK"
      },
      "source": [
        "## Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSfnk-r6Tq6E"
      },
      "outputs": [],
      "source": [
        "model_name = \"ResNet50\"\n",
        "checkpoint_path = os.path.join(project_root, f\"{model_name}_checkpoint_latest.pth\")\n",
        "train_val_path = os.path.join(project_root, f\"{model_name}_train_val_results.csv\")\n",
        "test_results_path = os.path.join(project_root, f\"{model_name}_test_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCQCIRkVRPw4",
        "outputId": "0f286c6a-1f31-4057-8d01-6f947b19f8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 51.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "res_model18=models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDeBZJ0yRXM4"
      },
      "outputs": [],
      "source": [
        "res_model18.fc= nn.Sequential(\n",
        "    nn.Dropout(p=0.5), # Look at changing to 0.6\n",
        "    nn.Linear(res_model18.fc.in_features, num_class))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2xurhPQhSA3",
        "outputId": "d76a1901-d024-4487-c74a-a3ac765ded1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 175MB/s]\n"
          ]
        }
      ],
      "source": [
        "res_model50 = models.resnet50(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_model50.fc= nn.Sequential(\n",
        "    nn.Dropout(p=0.5), # Look at changing to 0.6\n",
        "    nn.Linear(res_model50.fc.in_features, num_class))"
      ],
      "metadata": {
        "id": "vf5gnsK9dlmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1tWAny6g1IY"
      },
      "source": [
        "## Vision Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZz6ZEdDh7hj"
      },
      "source": [
        "used for medical imaging\n",
        "- robust to dataset imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoJMoLECh3qq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "de8bf76ff33043d5bef823ac6654c403",
            "93b610ae32ca405ab7c1e0a849e088e1",
            "6b8d7f0cf6e4492396b010463ad27a27",
            "faec40135f96487890be59c1ee866806",
            "92e42f74ab3f48daa428b6c3350be3d4",
            "5c99b3dc31a146f5a3a16cba8e307ed0",
            "85ed407d74094758a34f8c241275986d",
            "19c469e87ca8429c88cb37717575abc5",
            "fbd30029845748098af64fb23643fca1",
            "0d32d446238e4e20904a87f9ba9ade23",
            "fc933edcc25048969e8886a7bdec55b8"
          ]
        },
        "outputId": "9ee55e91-ef7f-4575-869b-edc1408e648c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de8bf76ff33043d5bef823ac6654c403"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import timm\n",
        "vis_model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4wIwkTRyizU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207a5179-32c5-4fef-e2de-b4946b82af52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionTransformer(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (patch_drop): Identity()\n",
            "  (norm_pre): Identity()\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (1): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (2): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (3): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (4): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (5): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (6): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (7): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (8): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (9): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (10): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (11): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  (fc_norm): Identity()\n",
            "  (head_drop): Dropout(p=0.0, inplace=False)\n",
            "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(vis_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5Wc2QQ4j1KJ"
      },
      "outputs": [],
      "source": [
        "vis_model.head = nn.Linear(vis_model.head.in_features, num_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfbfAYg2hPPh"
      },
      "source": [
        "## Efficient Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMUzJqvpieq2"
      },
      "source": [
        "excellent accuracy balance depth width and resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5462c44f398c4e128b0a66614facc5c5",
            "8e2acb9467574c968bf60303dc8b9765",
            "8a7cac3089f843fdb7ead87e3e5d2079",
            "e80418266fe044428832a4619024100b",
            "74c608bc96724f83976f44eec36b1f99",
            "077a7a0813d4478b9596992996acf158",
            "e8aece3f6ec04613b301e57fa8d62b52",
            "9a65524c1751416c8901e0f7110d735c",
            "822084c3993f40a9a59416c1e5487c00",
            "d4de38e0352b461db0e818fe37edd99f",
            "976ee0fbb034461b92be1935859ba4a6"
          ]
        },
        "id": "cuhg5tzkijiC",
        "outputId": "fec545b3-258e-4bfb-c8e0-5b6a19209427"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5462c44f398c4e128b0a66614facc5c5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import timm\n",
        "eff_model = timm.create_model(\"efficientnet_b3\",pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d7aFgAxj9LF"
      },
      "outputs": [],
      "source": [
        "eff_model.classifier =nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(eff_model.classifier.in_features, num_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWMejZDCr2yD"
      },
      "source": [
        "# Models running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evdnjctw0gnD",
        "outputId": "2d3f3394-bcf7-4244-9b5b-bb8f43bbeab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available()\\\n",
        "          else 'mps' if torch.mps.is_available()\\\n",
        "          else 'cpu'\n",
        "print('device', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e2M2Z5DIXzU",
        "outputId": "a2139883-75b6-4ae3-c64c-8ff18a3d6d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting focal-loss\n",
            "  Downloading focal_loss-0.0.7-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.12/dist-packages (from focal-loss) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2->focal-loss) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal-loss) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.2->focal-loss) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.2->focal-loss) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.2->focal-loss) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.2->focal-loss) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.2->focal-loss) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.2->focal-loss) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.2->focal-loss) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.2->focal-loss) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.2->focal-loss) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.2->focal-loss) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.2->focal-loss) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.2->focal-loss) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.2->focal-loss) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.2->focal-loss) (0.1.2)\n",
            "Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: focal-loss\n",
            "Successfully installed focal-loss-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install focal-loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrLn5BssLCWg",
        "outputId": "d8bef14d-9b12-434e-a9bf-18a9322c8506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: focal-loss\n",
            "Version: 0.0.7\n",
            "Summary: TensorFlow implementation of focal loss.\n",
            "Home-page: https://github.com/artemmavrin/focal-loss\n",
            "Author: Artem Mavrin\n",
            "Author-email: artemvmavrin@gmail.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: tensorflow\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show focal-loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zutwJyb4LThO"
      },
      "outputs": [],
      "source": [
        "# As using pytorch need class to use focal loss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        probs = torch.exp(log_probs)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).float()\n",
        "\n",
        "        focal_term = (1 - probs) ** self.gamma\n",
        "        loss = -targets_one_hot * focal_term * log_probs\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha = torch.tensor(self.alpha).to(inputs.device)\n",
        "            loss = loss * alpha\n",
        "\n",
        "        loss = loss.sum(dim=1)\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Uexgnh2sFZD"
      },
      "outputs": [],
      "source": [
        "#criteria = torch.nn.CrossEntropyLoss(label_smoothing=0.1) original attempt\n",
        "# Initially without class weights seems to be overfittinhg\n",
        "#class_weights = [class_weights_dict[i] for i in range(num_class)]\n",
        "#criteria = FocalLoss(gamma=2.0, alpha=class_weights)\n",
        "\n",
        "# Class weights computed per fold\n",
        "class_counts = Counter([dataset.targets[i] for i in train_idx])\n",
        "weights = [1.0 / class_counts[c] for c in range(len(classes))]\n",
        "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criteria = FocalLoss(gamma=2.0, alpha=weights_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn21czZEsh9L"
      },
      "source": [
        "#Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEBELi9-0t8b"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC, ConfusionMatrix, MeanMetric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import (\n",
        "    MulticlassAccuracy,\n",
        "    MulticlassPrecision,\n",
        "    MulticlassRecall,\n",
        "    MulticlassF1Score\n",
        ")"
      ],
      "metadata": {
        "id": "323aesiyKLjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-MpCLQ4D-Sb"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def one_train_epoch(model, optimizer):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Macro metrics\n",
        "    losses = MeanMetric().to(device)\n",
        "    acc = MulticlassAccuracy(num_classes=num_class, average='macro').to(device)\n",
        "    precision = MulticlassPrecision(num_classes=num_class, average='macro').to(device)\n",
        "    recall = MulticlassRecall(num_classes=num_class, average='macro').to(device)\n",
        "    f1_macro = MulticlassF1Score(num_classes=num_class, average='macro').to(device)\n",
        "\n",
        "    # Per-class F1\n",
        "    f1_per_class = MulticlassF1Score(num_classes=num_class, average=None).to(device)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    all_preds= []\n",
        "    all_targets=[]\n",
        "\n",
        "    for batch_idx, (X, Y) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X)\n",
        "        loss = criteria(preds, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        losses.update(loss.item(), X.size(0))\n",
        "        acc.update(preds, Y)\n",
        "        precision.update(preds, Y)\n",
        "        recall.update(preds, Y)\n",
        "        f1_macro.update(preds, Y)\n",
        "\n",
        "        all_preds.append(preds.argmax(dim=1))\n",
        "        all_targets.append(Y)\n",
        "\n",
        "    # Concatenate predictions and targets for per-class F1\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_targets = torch.cat(all_targets)\n",
        "    f1_class_scores = f1_per_class(all_preds, all_targets)\n",
        "\n",
        "\n",
        "    print(f\"Train epoch duration: {time.time() - start_time:.2f} seconds\")\n",
        "    for i, score in enumerate(f1_class_scores):\n",
        "        print(f\"Train F1 - Class {i}: {score:.4f}\")\n",
        "\n",
        "    return (\n",
        "        losses.compute().item(),\n",
        "        acc.compute().item(),\n",
        "        precision.compute().item(),\n",
        "        recall.compute().item(),\n",
        "        f1_macro.compute().item(),\n",
        "        f1_class_scores.cpu().numpy()  # Optional: return for logging or plotting\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lXsbdN9FRI7"
      },
      "outputs": [],
      "source": [
        "def one_val_epoch(model):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Macro metrics\n",
        "    losses = MeanMetric().to(device)\n",
        "    acc = MulticlassAccuracy(num_classes=num_class, average='macro').to(device)\n",
        "    precision = MulticlassPrecision(num_classes=num_class, average='macro').to(device)\n",
        "    recall = MulticlassRecall(num_classes=num_class, average='macro').to(device)\n",
        "    f1_macro = MulticlassF1Score(num_classes=num_class, average='macro').to(device)\n",
        "    auc = MulticlassAUROC(num_classes=num_class, average='macro').to(device)\n",
        "\n",
        "    # Per-class F1\n",
        "    f1_per_class = MulticlassF1Score(num_classes=num_class, average=None).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_preds=[]\n",
        "    all_targets=[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, Y in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            preds = model(X)\n",
        "            loss = criteria(preds, Y)\n",
        "\n",
        "            losses.update(loss.item(), X.size(0))\n",
        "            acc.update(preds, Y)\n",
        "            precision.update(preds, Y)\n",
        "            recall.update(preds, Y)\n",
        "            f1_macro.update(preds, Y)\n",
        "            auc.update(preds, Y)\n",
        "\n",
        "            all_preds.append(preds.argmax(dim=1))\n",
        "            all_targets.append(Y)\n",
        "\n",
        "    # Compute per-class F1\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_targets = torch.cat(all_targets)\n",
        "    f1_class_scores = f1_per_class(all_preds, all_targets)\n",
        "\n",
        "    print(f\"Validation took {time.time() - start_time:.2f}s\")\n",
        "    for i, score in enumerate(f1_class_scores):\n",
        "        print(f\"Val F1 - Class {i}: {score:.4f}\")\n",
        "\n",
        "    return (\n",
        "        losses.compute().item(),\n",
        "        acc.compute().item(),\n",
        "        precision.compute().item(),\n",
        "        recall.compute().item(),\n",
        "        f1_macro.compute().item(),\n",
        "        auc.compute().item(),\n",
        "        f1_class_scores.cpu().numpy()  # Optional: return for logging or plotting\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwNsFZmxN8_N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7gQQzP5XAwE"
      },
      "outputs": [],
      "source": [
        "def one_test_epoch(model):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Macro metrics\n",
        "    losses = MeanMetric().to(device)\n",
        "    acc = MulticlassAccuracy(num_classes=num_class, average='macro').to(device)\n",
        "    precision = MulticlassPrecision(num_classes=num_class, average='macro').to(device)\n",
        "    recall = MulticlassRecall(num_classes=num_class, average='macro').to(device)\n",
        "    f1_macro = MulticlassF1Score(num_classes=num_class, average='macro').to(device)\n",
        "    auc = MulticlassAUROC(num_classes=num_class, average='macro').to(device)\n",
        "\n",
        "    # Per-class F1\n",
        "    f1_per_class = MulticlassF1Score(num_classes=num_class, average=None).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    true_labels_log = []\n",
        "    predictions_log = []\n",
        "\n",
        "    all_preds=[]\n",
        "    all_targets=[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, Y in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            preds = model(X)\n",
        "            loss = criteria(preds, Y)\n",
        "\n",
        "            losses.update(loss.item(), X.size(0))\n",
        "            acc.update(preds, Y)\n",
        "            precision.update(preds, Y)\n",
        "            recall.update(preds, Y)\n",
        "            f1_macro.update(preds, Y)\n",
        "            auc.update(preds, Y)\n",
        "\n",
        "            pred_labels = preds.argmax(dim=1)\n",
        "            true_labels_log.extend(Y.cpu().numpy())\n",
        "            predictions_log.extend(pred_labels.cpu().numpy())\n",
        "\n",
        "            all_preds.append(pred_labels)\n",
        "            all_targets.append(Y)\n",
        "\n",
        "    # Compute per-class F1\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_targets = torch.cat(all_targets)\n",
        "    f1_class_scores = f1_per_class(all_preds, all_targets)\n",
        "\n",
        "    print(f\"Testing took {time.time() - start_time:.2f}s\")\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(true_labels_log, predictions_log, digits=4))\n",
        "\n",
        "    # Save full classification report\n",
        "    report_dict = classification_report(true_labels_log, predictions_log, digits=4, output_dict=True)\n",
        "    report_df = pd.DataFrame(report_dict).transpose()\n",
        "    report_df.to_csv(os.path.join(project_root, f\"{model_name}_class_report.csv\"))\n",
        "\n",
        "    # Save per-class metrics only (excluding summary rows)\n",
        "    per_class_metrics = report_df.drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore')\n",
        "    per_class_metrics.to_csv(os.path.join(project_root, f\"{model_name}_per_class_metrics.csv\"))\n",
        "\n",
        "    # Optional: Print per-class F1 from torchmetrics\n",
        "    print(\"\\nTorchMetrics Per-Class F1:\")\n",
        "    for i, score in enumerate(f1_class_scores):\n",
        "        print(f\"Test F1 - Class {i}: {score:.4f}\")\n",
        "\n",
        "    return (\n",
        "        losses.compute().item(),\n",
        "        acc.compute().item(),\n",
        "        precision.compute().item(),\n",
        "        recall.compute().item(),\n",
        "        f1_macro.compute().item(),\n",
        "        auc.compute().item(),\n",
        "        true_labels_log,\n",
        "        predictions_log,\n",
        "        f1_class_scores.cpu().numpy()  # Optional: return for logging or visualization\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_results=[]\n",
        "def run_training(model_name, model, optimizer, scheduler,\n",
        "                 train_val_path, test_results_path, checkpoint_path,\n",
        "                 num_epochs=30, patience=15, min_delta=0.003,\n",
        "                 minority_classes=[\"AK\", \"DF\", \"SCC\", \"VASC\"]):\n",
        "\n",
        "    model = model.to(device)\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    start_epoch = 0\n",
        "\n",
        "    # Resume from checkpoint if exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "        start_epoch = checkpoint.get(\"epoch\", 0) + 1\n",
        "        print(f\"{model_name} Resuming training from epoch {start_epoch}...\")\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        # === Train & Validate ===\n",
        "        train_loss, train_acc, train_prec, train_rec, train_f1, train_f1_per_class = one_train_epoch(model, optimizer)\n",
        "        val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, val_f1_per_class = one_val_epoch(model)\n",
        "\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Current LR: {current_lr:.6f}\")\n",
        "\n",
        "        # === Compute minority macro F1 ===\n",
        "        minority_idx = [classes.index(c) for c in minority_classes]\n",
        "        minority_macro_f1 = float(np.mean([val_f1_per_class[i] for i in minority_idx]))\n",
        "\n",
        "        # === Log metrics ===\n",
        "        metrics = {\n",
        "            \"Model\": model_name, \"Epoch\": epoch,\n",
        "            \"Train Loss\": train_loss, \"Train Accuracy\": train_acc,\n",
        "            \"Train Precision\": train_prec, \"Train Recall\": train_rec, \"Train F1\": train_f1,\n",
        "            \"Val Loss\": val_loss, \"Val Accuracy\": val_acc,\n",
        "            \"Val Precision\": val_prec, \"Val Recall\": val_rec, \"Val F1\": val_f1,\n",
        "            \"Val Minority Macro F1\": minority_macro_f1,\n",
        "            \"Val AUC\": val_auc\n",
        "        }\n",
        "        train_val_results.append(metrics)\n",
        "        pd.DataFrame([metrics]).to_csv(train_val_path, mode='a',\n",
        "                                       header=not os.path.exists(train_val_path),\n",
        "                                       index=False)\n",
        "\n",
        "        # === Log per-class F1s ===\n",
        "        f1_df = pd.DataFrame({\n",
        "            \"Epoch\": [epoch] * num_class,\n",
        "            \"Class\": list(range(num_class)),\n",
        "            \"Train F1\": train_f1_per_class,\n",
        "            \"Val F1\": val_f1_per_class\n",
        "        })\n",
        "        f1_df.to_csv(os.path.join(project_root, f\"{model_name}_f1_per_class_by_epoch.csv\"),\n",
        "                     mode='a', header=not os.path.exists(os.path.join(project_root, f\"{model_name}_f1_per_class_by_epoch.csv\")),\n",
        "                     index=False)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train F1: {train_f1:.4f} | \"\n",
        "              f\"Val F1: {val_f1:.4f} | Minority F1: {minority_macro_f1:.4f}\")\n",
        "\n",
        "        # === Early stopping on macro F1 ===\n",
        "        if val_f1 > best_val_f1 + min_delta:\n",
        "            best_val_f1 = val_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "            # Save best F1 checkpoint\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'tag': 'best_f1'\n",
        "            }, os.path.join(project_root, f\"{model_name}_best_f1.pth\"))\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping activated.\")\n",
        "                break\n",
        "\n",
        "        # Save periodic checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'tag': 'intermediate'\n",
        "            }, os.path.join(project_root, f\"{model_name}_checkpoint_epoch_{epoch}.pth\"))\n",
        "\n",
        "        # Always save latest\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'tag': 'latest'\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    # === Restore best F1 weights ===\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    print(f\"{model_name} Training complete. Running test set...\")\n",
        "\n",
        "    # === Test ===\n",
        "    test_loss, test_acc, test_prec, test_rec, test_f1, test_auc, y_true, y_pred, test_f1_per_class = one_test_epoch(model)\n",
        "\n",
        "    test_row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Test Loss\": test_loss, \"Test Accuracy\": test_acc,\n",
        "        \"Test Precision\": test_prec, \"Test Recall\": test_rec,\n",
        "        \"Test F1\": test_f1, \"Test AUC\": test_auc\n",
        "    }\n",
        "    pd.DataFrame([test_row]).to_csv(test_results_path, mode='a',\n",
        "                                    header=not os.path.exists(test_results_path),\n",
        "                                    index=False)\n",
        "\n",
        "    # Save predictions\n",
        "    pd.DataFrame({\"actual\": y_true, \"predicted\": y_pred}).to_csv(\n",
        "        os.path.join(project_root, f\"{model_name}_predictions.csv\"), index=False)\n",
        "\n",
        "    # Save per-class metrics\n",
        "    report_dict = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
        "    per_class_metrics = pd.DataFrame(report_dict).transpose().drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore')\n",
        "    per_class_metrics.to_csv(os.path.join(project_root, f\"{model_name}_per_class_metrics.csv\"))\n",
        "\n",
        "    # Save torchmetrics per-class F1\n",
        "    pd.DataFrame({\"Class\": list(range(num_class)), \"Test F1\": test_f1_per_class}).to_csv(\n",
        "        os.path.join(project_root, f\"{model_name}_test_f1_per_class.csv\"), index=False)\n",
        "\n",
        "    print(\"Test results saved, predictions log saved, and per-class metrics exported.\")\n"
      ],
      "metadata": {
        "id": "P9byry5hapHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLROeZVLqRuL"
      },
      "source": [
        "#Models to train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_resnet18(num_class):\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(p=0.5),  # you can change to 0.6 if you want more regularisation\n",
        "        nn.Linear(model.fc.in_features, num_class)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def make_efficientnet_b3(num_class):\n",
        "    model = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(model.classifier.in_features, num_class)\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "efvT4CHzdYrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Subset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.models as models  # for ResNet18 if you re-enable it\n",
        "\n",
        "\n",
        "# === MoCo (query-only) model ===\n",
        "class MoCoEffComboModel(nn.Module):\n",
        "    def __init__(self, encoder_q, encoder_k, queue, embedding_dim, alpha=0.5, num_class=3):\n",
        "        super().__init__()\n",
        "        self.encoder_q = encoder_q\n",
        "        self.encoder_k = encoder_k\n",
        "        for p in self.encoder_k.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.encoder_k.eval()\n",
        "\n",
        "        # Register queue as a buffer so .to(device) moves it automatically\n",
        "        self.register_buffer(\"queue\", queue)  # shape: [queue_size, embedding_dim]\n",
        "        self.alpha = alpha  # kept for compatibility, not used in query-only\n",
        "        self.classifier = nn.Linear(embedding_dim, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Query-only path, but still updates the momentum queue\n",
        "        query_features, _, self.queue = moco_forward(self.encoder_q, self.encoder_k, self.queue, x)\n",
        "\n",
        "        if query_features.dim() > 2:\n",
        "            query_features = F.adaptive_avg_pool2d(query_features, 1).view(query_features.size(0), -1)\n",
        "\n",
        "        # Align device/dtype with queue buffer\n",
        "        query_features = query_features.to(self.queue.device).to(self.queue.dtype)\n",
        "        return self.classifier(query_features)\n",
        "\n",
        "    def update_momentum(self, momentum=0.999):\n",
        "        update_momentum_encoder(self.encoder_q, self.encoder_k, momentum)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def update_momentum_encoder(encoder_q, encoder_k, momentum=0.999):\n",
        "    for param_q, param_k in zip(encoder_q.parameters(), encoder_k.parameters()):\n",
        "        param_k.data = param_k.data * momentum + param_q.data * (1. - momentum)\n",
        "\n",
        "\n",
        "def moco_forward(encoder_q, encoder_k, queue, x):\n",
        "    # Forward through query encoder (with grads)\n",
        "    query_features = encoder_q(x)\n",
        "\n",
        "    # Forward through momentum encoder (no grads, inference mode)\n",
        "    with torch.inference_mode():\n",
        "        # Ensure x is on the same device as encoder_k\n",
        "        x_k = x.to(next(encoder_k.parameters()).device, non_blocking=True)\n",
        "        mom_features = encoder_k(x_k)\n",
        "        # Move momentum features to queue device/dtype for concatenation\n",
        "        mom_features = mom_features.to(queue.device).to(queue.dtype)\n",
        "\n",
        "    # Update queue: prepend mom_features, keep fixed size\n",
        "    queue = torch.cat([mom_features, queue], dim=0)[: queue.shape[0]]\n",
        "    return query_features, mom_features, queue\n",
        "\n",
        "\n",
        "# === Model builders ===\n",
        "def make_resnet18(num_class):\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(model.fc.in_features, num_class)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_efficientnet_b3(num_class):\n",
        "    model = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(model.classifier.in_features, num_class)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_moco_effcombo_model(num_classes, queue_size=32, alpha=0.5, img_size=384):\n",
        "    # Create two EfficientNet-B0 encoders\n",
        "    encoder_q = timm.create_model(\"efficientnet_b0\", pretrained=True)\n",
        "    encoder_q.classifier = nn.Identity()\n",
        "\n",
        "    encoder_k = timm.create_model(\"efficientnet_b0\", pretrained=True)\n",
        "    encoder_k.classifier = nn.Identity()\n",
        "    encoder_k.eval()  # momentum encoder stays in eval\n",
        "\n",
        "    # Infer embedding dim using intended input resolution\n",
        "    dummy_input = torch.randn(1, 3, img_size, img_size)\n",
        "    with torch.no_grad():\n",
        "        dummy_output = encoder_q(dummy_input)\n",
        "        if dummy_output.dim() > 2:\n",
        "            dummy_output = F.adaptive_avg_pool2d(dummy_output, 1).view(1, -1)\n",
        "        embedding_dim = dummy_output.shape[1]\n",
        "\n",
        "    # Initialize queue (will be registered as buffer and moved with model.to(device))\n",
        "    queue = F.normalize(torch.randn(queue_size, embedding_dim), dim=1)\n",
        "\n",
        "    model = MoCoEffComboModel(\n",
        "        encoder_q=encoder_q,\n",
        "        encoder_k=encoder_k,\n",
        "        queue=queue,\n",
        "        embedding_dim=embedding_dim,\n",
        "        alpha=alpha,\n",
        "        num_class=num_classes\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# === Training setup ===\n",
        "models_to_train = [\n",
        "    (\"EfficientNetB3\", lambda num_classes: make_efficientnet_b3(num_classes)),\n",
        "    # (\"ResNet18\", lambda num_classes: make_resnet18(num_classes)),\n",
        "    (\"MoCoEffCombo\", lambda num_classes: build_moco_effcombo_model(num_classes, queue_size=32, alpha=0.5, img_size=384))\n",
        "]\n"
      ],
      "metadata": {
        "id": "CL0T3gS_J1RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(classes)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0LP8A-weMIO",
        "outputId": "1bc7e39e-d4d1-48f8-902d-0c3c27ada5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kQl-Kg1SAcq"
      },
      "outputs": [],
      "source": [
        "# Maybe add scheduler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "eQ8vGogmgFh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Subset, DataLoader, WeightedRandomSampler\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import gc\n",
        "\n",
        "\n",
        "for model_name, model_fn in models_to_train:\n",
        "    all_fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.arange(len(dataset)), dataset.targets)):\n",
        "        print(f\"\\n===== {model_name} | Fold {fold+1}/{n_splits} =====\")\n",
        "\n",
        "        # --- Fold-specific file paths ---\n",
        "        checkpoint_path = os.path.join(project_root, f\"{model_name}_fold{fold+1}_checkpoint_latest.pth\")\n",
        "        train_val_path = os.path.join(project_root, f\"{model_name}_fold{fold+1}_train_val_log.csv\")\n",
        "        test_results_path = os.path.join(project_root, f\"{model_name}_fold{fold+1}_test_results.csv\")\n",
        "\n",
        "        # --- Skip if results already exist ---\n",
        "        if os.path.exists(test_results_path):\n",
        "            print(f\"Skipping fold {fold+1} — results already exist\")\n",
        "            # Optionally, load the existing results into all_fold_results\n",
        "            existing_results = pd.read_csv(test_results_path)\n",
        "            all_fold_results.append(existing_results)\n",
        "            continue\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # --- Fresh model for this fold ---\n",
        "        model = model_fn(num_classes).to(device)\n",
        "\n",
        "\n",
        "        # --- Optimizer & scheduler ---\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
        "\n",
        "        # --- Build fold datasets ---\n",
        "        train_set = Subset(dataset, train_idx)\n",
        "        val_set   = Subset(dataset, val_idx)\n",
        "\n",
        "        # Apply transforms\n",
        "        train_set.dataset.transform = AlbTransform(train_augmentations)\n",
        "        val_set.dataset.transform = transforms.Compose([\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # --- Compute fold-specific class weights ---\n",
        "        class_counts = Counter([dataset.targets[i] for i in train_idx])\n",
        "        weights = [1.0 / class_counts[c] for c in range(len(classes))]\n",
        "        weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "        # --- Weighted sampler ---\n",
        "        sample_weights = [weights[label] for label in [dataset.targets[i] for i in train_idx]]\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "        # --- DataLoaders ---\n",
        "        globals()['train_loader'] = DataLoader(train_set, batch_size=16, sampler=sampler)\n",
        "        globals()['val_loader']   = DataLoader(val_set, batch_size=16, shuffle=False)\n",
        "\n",
        "        # --- Train this fold ---\n",
        "        run_training(\n",
        "            model_name=f\"{model_name}_fold{fold+1}\",\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            train_val_path=train_val_path,\n",
        "            test_results_path=test_results_path,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            num_epochs=30\n",
        "        )\n",
        "\n",
        "        # --- Collect fold results ---\n",
        "        fold_results = pd.read_csv(test_results_path)\n",
        "        all_fold_results.append(fold_results)\n",
        "\n",
        "        # Cleanup\n",
        "        del model, optimizer\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # === Average results across folds ===\n",
        "    avg_results = pd.concat(all_fold_results).mean(numeric_only=True)\n",
        "    avg_results[\"Model\"] = model_name\n",
        "    avg_results.to_csv(os.path.join(project_root, f\"{model_name}_cv_average_results.csv\"), index=False)\n",
        "    print(f\"\\n{model_name} CV average results:\\n\", avg_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn13gpVWFDxp",
        "outputId": "87050658-e201-4142-a852-f4aad7e1052b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== EfficientNetB3 | Fold 1/5 =====\n",
            "Skipping fold 1 — results already exist\n",
            "\n",
            "===== EfficientNetB3 | Fold 2/5 =====\n",
            "Skipping fold 2 — results already exist\n",
            "\n",
            "===== EfficientNetB3 | Fold 3/5 =====\n",
            "Skipping fold 3 — results already exist\n",
            "\n",
            "===== EfficientNetB3 | Fold 4/5 =====\n",
            "Skipping fold 4 — results already exist\n",
            "\n",
            "===== EfficientNetB3 | Fold 5/5 =====\n",
            "Skipping fold 5 — results already exist\n",
            "\n",
            "EfficientNetB3 CV average results:\n",
            " Test Loss               0.001232\n",
            "Test Accuracy           0.446752\n",
            "Test Precision          0.416494\n",
            "Test Recall             0.446752\n",
            "Test F1                 0.412846\n",
            "Test AUC                0.842988\n",
            "Model             EfficientNetB3\n",
            "dtype: object\n",
            "\n",
            "===== MoCoEffCombo | Fold 1/5 =====\n",
            "Skipping fold 1 — results already exist\n",
            "\n",
            "===== MoCoEffCombo | Fold 2/5 =====\n",
            "Skipping fold 2 — results already exist\n",
            "\n",
            "===== MoCoEffCombo | Fold 3/5 =====\n",
            "Skipping fold 3 — results already exist\n",
            "\n",
            "===== MoCoEffCombo | Fold 4/5 =====\n",
            "Skipping fold 4 — results already exist\n",
            "\n",
            "===== MoCoEffCombo | Fold 5/5 =====\n",
            "Skipping fold 5 — results already exist\n",
            "\n",
            "MoCoEffCombo CV average results:\n",
            " Test Loss             0.001407\n",
            "Test Accuracy         0.417481\n",
            "Test Precision        0.385674\n",
            "Test Recall           0.417481\n",
            "Test F1               0.384765\n",
            "Test AUC               0.82471\n",
            "Model             MoCoEffCombo\n",
            "dtype: object\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PDVTO9XBaEhM",
        "t4uBrs_dPHGH",
        "JlhzzakPPWtf",
        "OTxS3Q2eI7Lq",
        "32R4hWwnKC1g",
        "pTKAsDrIAKBJ",
        "Hh1OeY_J_3PD",
        "Lp_B9tP6gvxP",
        "pcX4WIIpgzEK",
        "u1tWAny6g1IY",
        "XfbfAYg2hPPh",
        "WWMejZDCr2yD",
        "kn21czZEsh9L",
        "oh1942Z6sQuw",
        "EHLYYdWYyOxs",
        "pSyl9lMeHj1k",
        "8wKWUeVZel89",
        "Mrb7hw2Oxsvp",
        "kWtMTBLI1d7j",
        "Id2AXLeD47XX",
        "dHdjpLea4-jo",
        "rzAhOovc5AYz",
        "Ym-3RrSw5Cre",
        "b3AQizhg5E72",
        "aozxJI3W5G_p",
        "vbzCnu7Z5Pa_",
        "K93U0ja55Q7m",
        "FxvE5ZITJOQE",
        "P1MF-fOPjgga"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de8bf76ff33043d5bef823ac6654c403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93b610ae32ca405ab7c1e0a849e088e1",
              "IPY_MODEL_6b8d7f0cf6e4492396b010463ad27a27",
              "IPY_MODEL_faec40135f96487890be59c1ee866806"
            ],
            "layout": "IPY_MODEL_92e42f74ab3f48daa428b6c3350be3d4"
          }
        },
        "93b610ae32ca405ab7c1e0a849e088e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c99b3dc31a146f5a3a16cba8e307ed0",
            "placeholder": "​",
            "style": "IPY_MODEL_85ed407d74094758a34f8c241275986d",
            "value": "model.safetensors: 100%"
          }
        },
        "6b8d7f0cf6e4492396b010463ad27a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19c469e87ca8429c88cb37717575abc5",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbd30029845748098af64fb23643fca1",
            "value": 346284714
          }
        },
        "faec40135f96487890be59c1ee866806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d32d446238e4e20904a87f9ba9ade23",
            "placeholder": "​",
            "style": "IPY_MODEL_fc933edcc25048969e8886a7bdec55b8",
            "value": " 346M/346M [00:01&lt;00:00, 165MB/s]"
          }
        },
        "92e42f74ab3f48daa428b6c3350be3d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c99b3dc31a146f5a3a16cba8e307ed0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85ed407d74094758a34f8c241275986d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19c469e87ca8429c88cb37717575abc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbd30029845748098af64fb23643fca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d32d446238e4e20904a87f9ba9ade23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc933edcc25048969e8886a7bdec55b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5462c44f398c4e128b0a66614facc5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e2acb9467574c968bf60303dc8b9765",
              "IPY_MODEL_8a7cac3089f843fdb7ead87e3e5d2079",
              "IPY_MODEL_e80418266fe044428832a4619024100b"
            ],
            "layout": "IPY_MODEL_74c608bc96724f83976f44eec36b1f99"
          }
        },
        "8e2acb9467574c968bf60303dc8b9765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_077a7a0813d4478b9596992996acf158",
            "placeholder": "​",
            "style": "IPY_MODEL_e8aece3f6ec04613b301e57fa8d62b52",
            "value": "model.safetensors: 100%"
          }
        },
        "8a7cac3089f843fdb7ead87e3e5d2079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a65524c1751416c8901e0f7110d735c",
            "max": 49335454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_822084c3993f40a9a59416c1e5487c00",
            "value": 49335454
          }
        },
        "e80418266fe044428832a4619024100b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4de38e0352b461db0e818fe37edd99f",
            "placeholder": "​",
            "style": "IPY_MODEL_976ee0fbb034461b92be1935859ba4a6",
            "value": " 49.3M/49.3M [00:00&lt;00:00, 171MB/s]"
          }
        },
        "74c608bc96724f83976f44eec36b1f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "077a7a0813d4478b9596992996acf158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8aece3f6ec04613b301e57fa8d62b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a65524c1751416c8901e0f7110d735c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822084c3993f40a9a59416c1e5487c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4de38e0352b461db0e818fe37edd99f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "976ee0fbb034461b92be1935859ba4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}